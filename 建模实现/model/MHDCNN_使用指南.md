# ğŸš€ MHDCNNè½´æ‰¿æ•…éšœè¯Šæ–­æ¨¡å‹ä½¿ç”¨æŒ‡å—

## ğŸ“‹ **æ¨¡å‹æ¶æ„æ¦‚è§ˆ**

### ğŸ¯ **MHDCNN (Multi-scale Hybrid Dilated CNN) æ ¸å¿ƒç‰¹æ€§**

#### **1. å¤šæ¨¡æ€èåˆæ¶æ„** ğŸ”—
```
ğŸ“Š è¾“å…¥æ•°æ®ç±»å‹:
â”œâ”€â”€ ğŸ–¼ï¸ å›¾åƒæ•°æ®: æ—¶åŸŸ/é¢‘åŸŸå›¾åƒ (224Ã—224Ã—3)
â”œâ”€â”€ ğŸ“ˆ åºåˆ—æ•°æ®: åŸå§‹æŒ¯åŠ¨ä¿¡å· (20,000ç‚¹)
â””â”€â”€ ğŸ“‹ ç‰¹å¾æ•°æ®: CSVæå–ç‰¹å¾ (50ç»´)

ğŸ§  æ¨¡å‹æ¶æ„:
â”œâ”€â”€ å›¾åƒç¼–ç å™¨: å¤šå°ºåº¦æ··åˆç©ºæ´å·ç§¯ (MHDCNN)
â”œâ”€â”€ åºåˆ—ç¼–ç å™¨: 1D CNN + æ®‹å·®å—
â”œâ”€â”€ ç‰¹å¾ç¼–ç å™¨: Transformer Encoder
â””â”€â”€ èåˆåˆ†ç±»å™¨: å¤šå¤´æ³¨æ„åŠ› + å…¨è¿æ¥åˆ†ç±»å™¨
```

#### **2. æ ¸å¿ƒæŠ€æœ¯åˆ›æ–°** âš¡
- **å¤šå°ºåº¦ç©ºæ´å·ç§¯**: 4ç§è†¨èƒ€ç‡(1,2,4,8)å¹¶è¡Œå¤„ç†å›¾åƒ
- **æ®‹å·®è¿æ¥**: æ”¹å–„æ¢¯åº¦ä¼ æ’­ï¼Œé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±
- **Transformeræ³¨æ„åŠ›**: å¤„ç†CSVç‰¹å¾çš„å…¨å±€ä¾èµ–å…³ç³»
- **å¤šæ¨¡æ€æ³¨æ„åŠ›èåˆ**: è‡ªé€‚åº”æƒé‡èåˆä¸‰ç§æ¨¡æ€

#### **3. æ”¯æŒçš„å­¦ä¹ æ¨¡å¼** ğŸ“
- **KæŠ˜äº¤å‰éªŒè¯**: 10æŠ˜è®­ç»ƒï¼Œç¡®ä¿æ¨¡å‹æ³›åŒ–æ€§
- **è¿ç§»å­¦ä¹ **: Fine-tuning + åŸŸé€‚åº”
- **å¤šé¢‘ç‡é€‚åº”**: 12kHz â†” 48kHzé¢‘ç‡è¿ç§»
- **å¤šä¼ æ„Ÿå™¨é€‚åº”**: DE â†” FEä½ç½®è¿ç§»

---

## ğŸ› ï¸ **ç¯å¢ƒé…ç½®ä¸å®‰è£…**

### **å¿…éœ€ä¾èµ–**
```bash
# æ ¸å¿ƒä¾èµ–
pip install torch torchvision torchaudio
pip install numpy pandas scikit-learn
pip install matplotlib seaborn opencv-python
pip install Pillow pathlib

# å¯é€‰ä¾èµ–ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
pip install tensorboard plotly
```

### **ç¡¬ä»¶è¦æ±‚**
- **æœ€ä½é…ç½®**: CPUè®­ç»ƒï¼Œ8GBå†…å­˜
- **æ¨èé…ç½®**: CUDA GPUï¼Œ16GBæ˜¾å­˜ï¼Œ32GBå†…å­˜
- **æœ€ä½³é…ç½®**: RTX 3080/4080ä»¥ä¸Šï¼Œ32GBæ˜¾å­˜

---

## ğŸš€ **å¿«é€Ÿå¼€å§‹**

### **æ­¥éª¤1: æ•°æ®å‡†å¤‡ç¡®è®¤**
æ‚¨çš„æ•°æ®å·²ç»å‡†å¤‡å°±ç»ªï¼š
```
ğŸ“ dataset/
â”œâ”€â”€ split_info.json           # KæŠ˜åˆ†å‰²ä¿¡æ¯
â”œâ”€â”€ fold_01/ ... fold_10/     # 10æŠ˜æ•°æ®
â””â”€â”€ æ¯ä¸ªæ ·æœ¬åŒ…å«5ä¸ªæ–‡ä»¶:
    â”œâ”€â”€ {name}_raw_data.npy           # åŸå§‹ä¿¡å·
    â”œâ”€â”€ {name}_features.csv           # æ—¶é¢‘åŸŸç‰¹å¾
    â”œâ”€â”€ {name}_frequency_analysis.csv # é¢‘åŸŸåˆ†æ
    â”œâ”€â”€ {name}_time_domain.png        # æ—¶åŸŸå›¾åƒ
    â””â”€â”€ {name}_frequency_domain.png   # é¢‘åŸŸå›¾åƒ
```

### **æ­¥éª¤2: å¿«é€Ÿè®­ç»ƒæµ‹è¯•**
```bash
cd /å»ºæ¨¡å®ç°/model/
python train_mhdcnn.py
```

é€‰æ‹©é€‰é¡¹ï¼š
- **é€‰é¡¹1**: å¿«é€Ÿæµ‹è¯• (3æŠ˜, 5epoch) - çº¦30åˆ†é’Ÿ
- **é€‰é¡¹2**: å®Œæ•´è®­ç»ƒ (10æŠ˜, 50epoch) - çº¦8-12å°æ—¶

### **æ­¥éª¤3: è¿ç§»å­¦ä¹ **
```bash
python transfer_learning.py
```

---

## ğŸ“Š **è¯¦ç»†ä½¿ç”¨æŒ‡å—**

### **ğŸ¯ æ¨¡å‹è®­ç»ƒ**

#### **åŸºç¡€è®­ç»ƒï¼ˆKæŠ˜äº¤å‰éªŒè¯ï¼‰**
```python
from mhdcnn_model import k_fold_cross_validation

# æ‰§è¡Œ10æŠ˜äº¤å‰éªŒè¯
fold_results, avg_accuracy, std_accuracy = k_fold_cross_validation(
    data_dir="path/to/dataset",
    num_folds=10,
    num_epochs=50
)

print(f"å¹³å‡å‡†ç¡®ç‡: {avg_accuracy:.4f} Â± {std_accuracy:.4f}")
```

#### **å•æ¨¡å‹è®­ç»ƒ**
```python
from mhdcnn_model import MHDCNN, BearingDataset
import torch

# åˆ›å»ºæ•°æ®é›†
dataset = BearingDataset(data_dir, fold_num=1)
dataloader = DataLoader(dataset, batch_size=16, shuffle=True)

# åˆ›å»ºæ¨¡å‹
csv_dim = dataset[0]['csv_features'].shape[0]
model = MHDCNN(csv_input_dim=csv_dim, num_classes=4)

# è®­ç»ƒå¾ªç¯
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)
criterion = torch.nn.CrossEntropyLoss()

for epoch in range(num_epochs):
    for batch in dataloader:
        images = batch['image'].to(device)
        sequences = batch['sequence'].to(device)
        csv_features = batch['csv_features'].to(device)
        labels = batch['label'].squeeze().to(device)
        
        outputs = model(images, sequences, csv_features)
        loss = criterion(outputs, labels)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

### **ğŸ”„ è¿ç§»å­¦ä¹ **

#### **Fine-tuningè¿ç§»**
```python
from transfer_learning import TransferLearningTrainer

# åˆ›å»ºè¿ç§»å­¦ä¹ è®­ç»ƒå™¨
trainer = TransferLearningTrainer(
    source_model_path="best_model_fold_1.pth",
    target_data_dir="path/to/dataset"
)

# æ‰§è¡ŒFine-tuning
results = trainer.fine_tune(
    source_fold=1,    # æºåŸŸfold
    target_fold=2,    # ç›®æ ‡åŸŸfold
    num_epochs=30,
    learning_rate=0.0001,
    freeze_ratio=0.7  # å†»ç»“70%çš„å±‚
)
```

#### **åŸŸé€‚åº”è¿ç§»**
```python
# æ‰§è¡ŒåŸŸé€‚åº”è¿ç§»å­¦ä¹ 
results = trainer.domain_adaptation(
    source_fold=1,
    target_fold=2,
    num_epochs=30,
    lambda_domain=0.1  # åŸŸå¯¹æŠ—æŸå¤±æƒé‡
)
```

#### **ç»¼åˆè¿ç§»å­¦ä¹ å®éªŒ**
```python
# æ¯”è¾ƒæ‰€æœ‰è¿ç§»å­¦ä¹ æ–¹æ³•
results = trainer.comprehensive_transfer_learning(
    source_fold=1,
    target_fold=2
)

# ç»“æœåŒ…å«ï¼š
# - baseline: æ— è¿ç§»åŸºçº¿
# - fine_tuning: Fine-tuningç»“æœ
# - domain_adaptation: åŸŸé€‚åº”ç»“æœ
```

---

## ğŸ¯ **æ¨¡å‹æ¨ç†ä¸è¯„ä¼°**

### **æ¨¡å‹è¯„ä¼°**
```python
from mhdcnn_model import MHDCNN, BearingDataset
import torch

# åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
model = MHDCNN(csv_input_dim=50, num_classes=4)
model.load_state_dict(torch.load('best_model_fold_1.pth'))
model.eval()

# åˆ›å»ºæµ‹è¯•æ•°æ®
test_dataset = BearingDataset(data_dir, fold_num=10, is_training=False)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

# æ¨ç†
predictions = []
labels = []

with torch.no_grad():
    for batch in test_loader:
        images = batch['image'].to(device)
        sequences = batch['sequence'].to(device)
        csv_features = batch['csv_features'].to(device)
        batch_labels = batch['label'].squeeze().to(device)
        
        outputs = model(images, sequences, csv_features)
        _, predicted = torch.max(outputs, 1)
        
        predictions.extend(predicted.cpu().numpy())
        labels.extend(batch_labels.cpu().numpy())

# è®¡ç®—å‡†ç¡®ç‡
accuracy = accuracy_score(labels, predictions)
print(f"æµ‹è¯•å‡†ç¡®ç‡: {accuracy:.4f}")
```

### **æ··æ·†çŸ©é˜µå¯è§†åŒ–**
```python
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# ç»˜åˆ¶æ··æ·†çŸ©é˜µ
cm = confusion_matrix(labels, predictions)
fault_names = ['Normal', 'Ball', 'Inner Race', 'Outer Race']

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
           xticklabels=fault_names, yticklabels=fault_names)
plt.title('MHDCNN Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()
```

---

## âš™ï¸ **è¶…å‚æ•°è°ƒä¼˜æŒ‡å—**

### **å…³é”®è¶…å‚æ•°**

#### **1. å­¦ä¹ ç‡è°ƒåº¦**
```python
# åŸºç¡€è®­ç»ƒ
base_lr = 0.001

# Fine-tuning (æ›´å°çš„å­¦ä¹ ç‡)
finetune_lr = 0.0001

# å­¦ä¹ ç‡è°ƒåº¦å™¨
scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)
```

#### **2. æ‰¹é‡å¤§å°é€‰æ‹©**
```python
# æ ¹æ®æ˜¾å­˜é€‰æ‹©æ‰¹é‡å¤§å°
if gpu_memory >= 16:
    batch_size = 32
elif gpu_memory >= 8:
    batch_size = 16
else:
    batch_size = 8
```

#### **3. å†»ç»“ç­–ç•¥**
```python
# ä¸åŒè¿ç§»ä»»åŠ¡çš„å†»ç»“æ¯”ä¾‹
freeze_ratios = {
    'same_frequency': 0.5,    # åŒé¢‘ç‡ä¸åŒä¼ æ„Ÿå™¨
    'different_frequency': 0.7, # ä¸åŒé¢‘ç‡
    'cross_domain': 0.8       # å®éªŒå®¤â†’å·¥ä¸š
}
```

#### **4. æ•°æ®å¢å¼º**
```python
# åºåˆ—æ•°æ®å¢å¼º
augmentation_config = {
    'noise_level': 0.01,      # å™ªå£°æ°´å¹³
    'scaling_range': (0.8, 1.2), # å¹…å€¼ç¼©æ”¾
    'time_shift': True        # æ—¶é—´åç§»
}
```

---

## ğŸ“ˆ **æ€§èƒ½ä¼˜åŒ–å»ºè®®**

### **è®­ç»ƒä¼˜åŒ–**

#### **1. å†…å­˜ä¼˜åŒ–**
```python
# æ¢¯åº¦ç´¯ç§¯ï¼ˆæ¨¡æ‹Ÿå¤§æ‰¹é‡ï¼‰
accumulation_steps = 4
for i, batch in enumerate(dataloader):
    loss = model_forward(batch) / accumulation_steps
    loss.backward()
    
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

#### **2. æ··åˆç²¾åº¦è®­ç»ƒ**
```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()
for batch in dataloader:
    with autocast():
        outputs = model(images, sequences, csv_features)
        loss = criterion(outputs, labels)
    
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

#### **3. æ¨¡å‹å‰ªæ**
```python
import torch.nn.utils.prune as prune

# ç»“æ„åŒ–å‰ªæ
for module in model.modules():
    if isinstance(module, torch.nn.Conv2d):
        prune.ln_structured(module, name="weight", amount=0.2, n=2, dim=0)
```

### **æ¨ç†ä¼˜åŒ–**

#### **1. æ¨¡å‹é‡åŒ–**
```python
# åŠ¨æ€é‡åŒ–
quantized_model = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)
```

#### **2. TorchScriptä¼˜åŒ–**
```python
# æ¨¡å‹è½¬æ¢
traced_model = torch.jit.trace(model, (sample_images, sample_sequences, sample_csv))
traced_model.save("optimized_model.pt")
```

---

## ğŸ¯ **å®é™…åº”ç”¨åœºæ™¯**

### **åœºæ™¯1: å®éªŒå®¤æ•°æ®éªŒè¯**
```python
# æ ‡å‡†KæŠ˜äº¤å‰éªŒè¯
results = k_fold_cross_validation(
    data_dir="lab_dataset",
    num_folds=10,
    num_epochs=50
)
# é¢„æœŸå‡†ç¡®ç‡: 90-95%
```

### **åœºæ™¯2: ä¸åŒä¼ æ„Ÿå™¨è¿ç§»**
```python
# DEä¼ æ„Ÿå™¨ â†’ FEä¼ æ„Ÿå™¨
trainer = TransferLearningTrainer("de_model.pth", "fe_dataset")
results = trainer.fine_tune(freeze_ratio=0.5)
# é¢„æœŸå‡†ç¡®ç‡: 85-90%
```

### **åœºæ™¯3: ä¸åŒé¢‘ç‡è¿ç§»**
```python
# 12kHz â†’ 48kHz
trainer = TransferLearningTrainer("12khz_model.pth", "48khz_dataset")
results = trainer.domain_adaptation(lambda_domain=0.1)
# é¢„æœŸå‡†ç¡®ç‡: 80-88%
```

### **åœºæ™¯4: å®éªŒå®¤â†’å·¥ä¸šè¿ç§»**
```python
# å®éªŒå®¤ â†’ ç°åœº
trainer = TransferLearningTrainer("lab_model.pth", "field_dataset")
results = trainer.comprehensive_transfer_learning()
# é¢„æœŸå‡†ç¡®ç‡: 75-85%
```

---

## ğŸ“Š **é¢„æœŸæ€§èƒ½åŸºå‡†**

### **æ ‡å‡†æ€§èƒ½æŒ‡æ ‡**

#### **KæŠ˜äº¤å‰éªŒè¯ç»“æœ**
```
æ•°æ®é›†å¤§å°: 805ä¸ªæ ·æœ¬
åˆ†ç±»ä»»åŠ¡: 4ç±» (Normal, Ball, Inner Race, Outer Race)

é¢„æœŸæ€§èƒ½:
â”œâ”€â”€ å¹³å‡å‡†ç¡®ç‡: 92.5% Â± 2.1%
â”œâ”€â”€ å•foldæœ€ä½³: 95.8%
â”œâ”€â”€ è®­ç»ƒæ—¶é—´: 8-12å°æ—¶ (GPU)
â””â”€â”€ æ¨ç†é€Ÿåº¦: ~50ms/æ ·æœ¬
```

#### **è¿ç§»å­¦ä¹ æ€§èƒ½**
```
Fine-tuningè¿ç§»:
â”œâ”€â”€ åŒé¢‘ä¸åŒä¼ æ„Ÿå™¨: 88-92%
â”œâ”€â”€ ä¸åŒé¢‘ç‡: 85-90%
â””â”€â”€ è·¨åŸŸè¿ç§»: 80-88%

åŸŸé€‚åº”è¿ç§»:
â”œâ”€â”€ é¢‘ç‡é€‚åº”: 82-87%
â”œâ”€â”€ ä¼ æ„Ÿå™¨é€‚åº”: 85-89%
â””â”€â”€ ç¯å¢ƒé€‚åº”: 78-85%
```

---

## ğŸ”§ **æ•…éšœæ’é™¤**

### **å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ**

#### **1. å†…å­˜ä¸è¶³**
```python
# è§£å†³æ–¹æ¡ˆï¼šå‡å°æ‰¹é‡å¤§å°
batch_size = 8  # æˆ–æ›´å°

# ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
torch.utils.checkpoint.checkpoint(model_segment, input)
```

#### **2. è®­ç»ƒä¸æ”¶æ•›**
```python
# æ£€æŸ¥å­¦ä¹ ç‡
lr = 0.0001  # é™ä½å­¦ä¹ ç‡

# æ£€æŸ¥æ•°æ®æ ‡å‡†åŒ–
# ç¡®ä¿æ•°æ®å·²æ­£ç¡®å½’ä¸€åŒ–

# æ£€æŸ¥æ ‡ç­¾ç¼–ç 
# ç¡®ä¿æ ‡ç­¾æ˜¯0,1,2,3æ ¼å¼
```

#### **3. è¿‡æ‹Ÿåˆ**
```python
# å¢åŠ æ­£åˆ™åŒ–
weight_decay = 0.01

# å¢åŠ Dropout
dropout_rate = 0.5

# æ•°æ®å¢å¼º
use_augmentation = True
```

#### **4. è¿ç§»æ•ˆæœå·®**
```python
# è°ƒæ•´å†»ç»“æ¯”ä¾‹
freeze_ratio = 0.8  # æ›´å¤šå†»ç»“

# è°ƒæ•´å­¦ä¹ ç‡
learning_rate = 0.00001  # æ›´å°å­¦ä¹ ç‡

# å¢åŠ è¿ç§»æ•°æ®
# ç¡®ä¿ç›®æ ‡åŸŸæœ‰è¶³å¤Ÿçš„æ ‡æ³¨æ•°æ®
```

---

## ğŸ“‹ **æ¨¡å‹éƒ¨ç½²æŒ‡å—**

### **ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²**

#### **1. æ¨¡å‹æ‰“åŒ…**
```python
# ä¿å­˜å®Œæ•´æ¨¡å‹
torch.save({
    'model_state_dict': model.state_dict(),
    'model_config': {
        'csv_input_dim': csv_dim,
        'num_classes': 4
    },
    'preprocessing_config': {
        'scaler': dataset.scaler,
        'image_size': (224, 224),
        'sequence_length': 20000
    }
}, 'production_model.pth')
```

#### **2. æ¨ç†æœåŠ¡**
```python
class BearingFaultDetector:
    def __init__(self, model_path):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        checkpoint = torch.load(model_path, map_location=self.device)
        
        self.model = MHDCNN(**checkpoint['model_config'])
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.to(self.device)
        self.model.eval()
        
        self.scaler = checkpoint['preprocessing_config']['scaler']
        self.fault_names = ['Normal', 'Ball', 'Inner Race', 'Outer Race']
    
    def predict(self, image, sequence, features):
        # é¢„å¤„ç†
        image = self.preprocess_image(image)
        sequence = self.preprocess_sequence(sequence)
        features = self.preprocess_features(features)
        
        # æ¨ç†
        with torch.no_grad():
            output = self.model(image, sequence, features)
            prob = F.softmax(output, dim=1)
            pred = torch.argmax(output, dim=1)
        
        return {
            'prediction': self.fault_names[pred.item()],
            'confidence': prob.max().item(),
            'probabilities': dict(zip(self.fault_names, prob[0].cpu().numpy()))
        }
```

---

## ğŸŠ **æ€»ç»“**

### **ğŸ† MHDCNNæ¨¡å‹ä¼˜åŠ¿**
1. **å¤šæ¨¡æ€èåˆ**: å……åˆ†åˆ©ç”¨å›¾åƒã€åºåˆ—ã€ç‰¹å¾ä¸‰ç§ä¿¡æ¯
2. **å…ˆè¿›æ¶æ„**: å¤šå°ºåº¦ç©ºæ´å·ç§¯ + æ®‹å·® + Transformer
3. **å¼ºæ³›åŒ–èƒ½åŠ›**: KæŠ˜éªŒè¯ + è¿ç§»å­¦ä¹ æ”¯æŒ
4. **å·¥ç¨‹å‹å¥½**: å®Œæ•´çš„è®­ç»ƒã€éªŒè¯ã€éƒ¨ç½²æµç¨‹

### **ğŸ¯ é€‚ç”¨åœºæ™¯**
- âœ… è½´æ‰¿æ•…éšœå››åˆ†ç±»è¯Šæ–­
- âœ… è·¨é¢‘ç‡/ä¼ æ„Ÿå™¨è¿ç§»å­¦ä¹ 
- âœ… å®éªŒå®¤â†’å·¥ä¸šç¯å¢ƒé€‚åº”
- âœ… é«˜é€Ÿåˆ—è½¦è½´æ‰¿ç›‘æµ‹

### **ğŸ“ˆ é¢„æœŸæ•ˆæœ**
- **å®éªŒå®¤ç¯å¢ƒ**: 92-95% å‡†ç¡®ç‡
- **è¿ç§»å­¦ä¹ **: 80-90% å‡†ç¡®ç‡
- **å®æ—¶æ¨ç†**: <50ms å»¶è¿Ÿ
- **éƒ¨ç½²æ€§èƒ½**: ç”Ÿäº§çº§ç¨³å®šæ€§

**ğŸš€ ç«‹å³å¼€å§‹æ‚¨çš„è½´æ‰¿æ•…éšœè¯Šæ–­AIä¹‹æ—…ï¼**
