#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ€ç»ˆæ•°æ®å¤„ç†å™¨ - ä¸¥æ ¼æŒ‰ç…§è¦æ±‚å®ç°
ç›®æ ‡ï¼šä»161ä¸ªæ–‡ä»¶ç”Ÿæˆ483ä¸ªæ•°æ®ç‰‡æ®µï¼ˆæ¯ä¸ªæ–‡ä»¶3ä¸ªç‰‡æ®µï¼‰
è¦æ±‚ï¼šç‰‡æ®µåˆ©ç”¨ç‡â‰¥60%ï¼Œä¿è¯æ•°æ®å¯¹é½å’Œè´¨é‡
"""

import os
import numpy as np
import scipy.io as sio
import scipy.signal as signal
import matplotlib.pyplot as plt
import pandas as pd
from pathlib import Path
import json
from datetime import datetime

class FinalDataProcessor:
    """æœ€ç»ˆæ•°æ®å¤„ç†å™¨ - ä¸¥æ ¼æŒ‰ç…§è¦æ±‚å®ç°"""
    
    def __init__(self, source_base_path, output_base_path):
        self.source_base = Path(source_base_path)
        self.output_base = Path(output_base_path)
        self.processed_count = 0
        self.global_folder_counter = 1  # å…¨å±€æ–‡ä»¶å¤¹è®¡æ•°å™¨
        
        # è½´æ‰¿å‚æ•°
        self.bearing_params = {
            'SKF6205': {'n': 9, 'd': 0.3126, 'D': 1.537},  # DEè½´æ‰¿
            'SKF6203': {'n': 9, 'd': 0.2656, 'D': 1.122}   # FEè½´æ‰¿
        }
    
    def load_mat_file(self, file_path):
        """åŠ è½½.matæ–‡ä»¶"""
        try:
            return sio.loadmat(str(file_path))
        except Exception as e:
            print(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return None
    
    def downsample_signal(self, data, original_fs, target_fs):
        """é™é‡‡æ ·ä¿¡å·"""
        if original_fs == target_fs:
            return data
        
        # è®¡ç®—é™é‡‡æ ·æ¯”ä¾‹
        downsample_factor = int(original_fs / target_fs)
        
        # ä½¿ç”¨scipyçš„decimateå‡½æ•°è¿›è¡Œé™é‡‡æ ·
        downsampled = signal.decimate(data.flatten(), downsample_factor, ftype='iir')
        
        return downsampled.reshape(-1, 1)
    
    def apply_denoising(self, data, fs):
        """åº”ç”¨å»å™ªæ»¤æ³¢å™¨"""
        data_flat = data.flatten()
        
        # 1. é«˜é€šæ»¤æ³¢ (10Hz)
        sos_hp = signal.butter(4, 10, btype='highpass', fs=fs, output='sos')
        data_filtered = signal.sosfilt(sos_hp, data_flat)
        
        # 2. ä½é€šæ»¤æ³¢ (5000Hz)
        sos_lp = signal.butter(4, 5000, btype='lowpass', fs=fs, output='sos')
        data_filtered = signal.sosfilt(sos_lp, data_filtered)
        
        # 3. é™·æ³¢æ»¤æ³¢ (50Hzå·¥é¢‘åŠå…¶è°æ³¢)
        for freq in [50, 100, 150]:
            if freq < fs/2:
                b_notch, a_notch = signal.iirnotch(freq, 30, fs)
                data_filtered = signal.filtfilt(b_notch, a_notch, data_filtered)
        
        # 4. ä¸­å€¼æ»¤æ³¢å»é™¤è„‰å†²å™ªå£°
        data_filtered = signal.medfilt(data_filtered, kernel_size=3)
        
        return data_filtered.reshape(-1, 1)
    
    def extract_three_segments_with_high_utilization(self, data, target_fs):
        """
        ä»æ•°æ®ä¸­æå–3ä¸ªç‰‡æ®µï¼Œç¡®ä¿é«˜åˆ©ç”¨ç‡ï¼ˆâ‰¥60%ï¼‰
        ç­–ç•¥ï¼šå‡åŒ€åˆ†å¸ƒæå–ï¼Œä¿è¯è¦†ç›–æ•´ä¸ªä¿¡å·
        """
        data_length = len(data)
        
        # è®¡ç®—ç›®æ ‡ç‰‡æ®µé•¿åº¦ï¼Œç¡®ä¿åˆ©ç”¨ç‡â‰¥60%
        min_utilization = 0.6
        min_total_points = int(data_length * min_utilization)
        target_points_per_segment = min_total_points // 3
        
        # ä½†æ˜¯è¦æ»¡è¶³åŸºæœ¬è´¨é‡è¦æ±‚
        if target_fs == 12000:
            min_segment_length = 20000  # 12kHzè‡³å°‘2ä¸‡ç‚¹
        else:
            min_segment_length = 60000  # 48kHzåŸå§‹è‡³å°‘6ä¸‡ç‚¹ï¼ˆé™é‡‡æ ·å1.5ä¸‡ç‚¹ï¼‰
        
        # é€‰æ‹©è¾ƒå¤§çš„å€¼ä½œä¸ºå®é™…æ®µé•¿åº¦
        segment_length = int(max(target_points_per_segment, min_segment_length))
        
        # å¦‚æœå•ä¸ªæ®µé•¿åº¦å¤ªå¤§ï¼Œè°ƒæ•´ä¸ºå¯è¡Œçš„æœ€å¤§å€¼
        max_possible_length = int(data_length // 3)
        segment_length = int(min(segment_length, max_possible_length))
        
        # ç¡®ä¿è‡³å°‘æœ‰æœ€å°å¯ç”¨é•¿åº¦
        if segment_length < min_segment_length * 0.8:  # å…è®¸20%çš„é™çº§
            segment_length = int(min_segment_length * 0.8)
        
        # è®¡ç®—ä¸‰ä¸ªæ®µçš„èµ·å§‹ä½ç½®ï¼Œå‡åŒ€åˆ†å¸ƒ
        # ç¬¬1æ®µï¼šä»å¤´å¼€å§‹
        # ç¬¬2æ®µï¼šä»ä¸­é—´å¼€å§‹  
        # ç¬¬3æ®µï¼šä»åé¢å¼€å§‹
        remaining_length = data_length - 3 * segment_length
        gap = remaining_length // 2 if remaining_length > 0 else 0
        
        segments = []
        positions = [
            0,  # ç¬¬1æ®µï¼šå¼€å¤´
            int(gap + segment_length),  # ç¬¬2æ®µï¼šä¸­é—´åå‰
            int(data_length - segment_length)  # ç¬¬3æ®µï¼šç»“å°¾
        ]
        
        for i, start_pos in enumerate(positions):
            start_pos = int(start_pos)
            end_pos = int(start_pos + segment_length)
            if end_pos > data_length:
                end_pos = int(data_length)
                start_pos = int(max(0, end_pos - segment_length))
            
            segment = data[start_pos:end_pos]
            if len(segment) >= segment_length * 0.9:  # å…è®¸10%çš„é•¿åº¦è¯¯å·®
                segments.append(segment)
            else:
                # å¦‚æœæ®µå¤ªçŸ­ï¼Œä»å½“å‰ä½ç½®å–æœ€å¤§å¯èƒ½é•¿åº¦
                available_length = int(data_length - start_pos)
                if available_length > segment_length * 0.5:
                    segments.append(data[start_pos:])
        
        # ç¡®ä¿æ­£å¥½è¿”å›3ä¸ªæ®µ
        while len(segments) < 3:
            # å¦‚æœæ®µæ•°ä¸å¤Ÿï¼Œä»å‰©ä½™æ•°æ®ä¸­è¡¥å……
            if len(segments) == 0:
                # æç«¯æƒ…å†µï¼šå‡åŒ€åˆ†å‰²
                seg_len = int(data_length // 3)
                segments = [
                    data[0:seg_len],
                    data[seg_len:2*seg_len], 
                    data[2*seg_len:]
                ]
            else:
                # å¤åˆ¶æœ€åä¸€ä¸ªæ®µï¼ˆå¸¦åç§»ï¼‰
                last_segment = segments[-1]
                offset = int(len(last_segment) // 2)
                start_pos = int(len(data) - len(last_segment) - offset)
                if start_pos < 0:
                    start_pos = 0
                end_pos = int(start_pos + len(last_segment))
                segments.append(data[start_pos:end_pos])
        
        # åªè¿”å›å‰3ä¸ªæ®µ
        segments = segments[:3]
        
        # è®¡ç®—å®é™…åˆ©ç”¨ç‡
        total_used_points = sum(len(seg) for seg in segments)
        utilization_rate = total_used_points / data_length
        
        print(f"    æ•°æ®é•¿åº¦: {data_length:,}, ç‰‡æ®µé•¿åº¦: {[len(s) for s in segments]}")
        print(f"    åˆ©ç”¨ç‡: {utilization_rate:.1%} (ç›®æ ‡â‰¥60%)")
        
        return segments, utilization_rate
    
    def extract_time_features(self, segment_data):
        """æå–æ—¶åŸŸç‰¹å¾"""
        data = segment_data.flatten()
        
        features = {
            'P1_Mean': float(np.mean(data)),
            'P2_RMS': float(np.sqrt(np.mean(data**2))),
            'P3_Variance': float(np.var(data)),
            'P4_Std': float(np.std(data)),
            'P5_Skewness': float(self._skewness(data)),
            'P6_Kurtosis': float(self._kurtosis(data)),
            'P7_Peak': float(np.max(np.abs(data))),
            'P8_Peak2Peak': float(np.max(data) - np.min(data)),
            'P9_CrestFactor': float(np.max(np.abs(data)) / np.sqrt(np.mean(data**2))),
            'P10_ShapeFactor': float(np.sqrt(np.mean(data**2)) / np.mean(np.abs(data))),
            'P11_ImpulseFactor': float(np.max(np.abs(data)) / np.mean(np.abs(data))),
            'P12_MarginFactor': float(np.max(np.abs(data)) / (np.mean(np.sqrt(np.abs(data))))**2),
            'P13_Energy': float(np.sum(data**2)),
            'P14_Entropy': float(self._calculate_entropy(data)),
            'P15_ZeroCrossing': int(np.sum(np.diff(np.sign(data)) != 0)),
            'P16_MeanFreq': float(self._mean_frequency(data))
        }
        
        return features
    
    def compute_fft_features(self, segment_data, fs):
        """è®¡ç®—FFTç‰¹å¾"""
        data = segment_data.flatten()
        
        # è®¡ç®—FFT
        fft_data = np.fft.fft(data)
        fft_magnitude = np.abs(fft_data[:len(fft_data)//2])
        frequencies = np.fft.fftfreq(len(data), 1/fs)[:len(fft_data)//2]
        
        # é¢‘åŸŸç‰¹å¾
        freq_mean = float(np.sum(frequencies * fft_magnitude) / np.sum(fft_magnitude))
        freq_var = float(np.sum((frequencies - freq_mean)**2 * fft_magnitude) / np.sum(fft_magnitude))
        
        features = {
            'P17_FreqMean': freq_mean,
            'P18_FreqVar': freq_var,
            'P19_FreqStd': float(np.sqrt(freq_var)),
            'P20_FreqSkew': float(self._freq_skewness(frequencies, fft_magnitude)),
            'P21_FreqKurt': float(self._freq_kurtosis(frequencies, fft_magnitude)),
            'P22_FreqRMS': float(np.sqrt(np.mean(fft_magnitude**2))),
            'P23_SpectralCentroid': float(np.sum(frequencies * fft_magnitude) / np.sum(fft_magnitude)),
            'P24_SpectralRolloff': float(self._spectral_rolloff(frequencies, fft_magnitude)),
            'P25_SpectralFlatness': float(self._spectral_flatness(fft_magnitude)),
            'P26_SpectralBandwidth': float(self._spectral_bandwidth(frequencies, fft_magnitude)),
            'P27_DominantFreq': float(frequencies[np.argmax(fft_magnitude)]),
            'P28_FreqPeak': float(np.max(fft_magnitude)),
            'P29_TotalPower': float(np.sum(fft_magnitude**2))
        }
        
        return features, fft_magnitude, frequencies
    
    def calculate_theoretical_fault_frequencies(self, rpm, bearing_type):
        """è®¡ç®—ç†è®ºæ•…éšœé¢‘ç‡"""
        fr = rpm / 60  # è½¬é¢‘(Hz)
        
        if bearing_type not in self.bearing_params:
            return {}
        
        params = self.bearing_params[bearing_type]
        n, d, D = params['n'], params['d'], params['D']
        
        # è®¡ç®—ç‰¹å¾é¢‘ç‡
        bpfo = (n * fr / 2) * (1 - d * np.cos(0) / D)  # å¤–åœˆæ•…éšœé¢‘ç‡
        bpfi = (n * fr / 2) * (1 + d * np.cos(0) / D)  # å†…åœˆæ•…éšœé¢‘ç‡
        bsf = (D * fr / (2 * d)) * (1 - (d * np.cos(0) / D)**2)  # æ»šåŠ¨ä½“æ•…éšœé¢‘ç‡
        ftf = (fr / 2) * (1 - d * np.cos(0) / D)  # ä¿æŒæ¶æ•…éšœé¢‘ç‡
        
        return {
            'BPFO': float(bpfo),
            'BPFI': float(bpfi),
            'BSF': float(bsf),
            'FTF': float(ftf),
            'FR': float(fr)
        }
    
    def analyze_frequency_components(self, fft_magnitude, frequencies, fs, rpm, bearing_type):
        """åˆ†æé¢‘ç‡æˆåˆ†"""
        # ä¸»é¢‘æ£€æµ‹
        peak_indices, _ = signal.find_peaks(fft_magnitude, height=np.max(fft_magnitude)*0.1)
        
        if len(peak_indices) > 0:
            main_peak_idx = peak_indices[np.argmax(fft_magnitude[peak_indices])]
            main_frequency = frequencies[main_peak_idx]
            main_amplitude = fft_magnitude[main_peak_idx]
        else:
            main_frequency = frequencies[np.argmax(fft_magnitude)]
            main_amplitude = np.max(fft_magnitude)
        
        # è°æ³¢æ£€æµ‹
        harmonics = []
        for h in range(2, 6):  # æ£€æµ‹2-5æ¬¡è°æ³¢
            harmonic_freq = main_frequency * h
            if harmonic_freq < fs/2:
                # æŸ¥æ‰¾è°æ³¢é™„è¿‘çš„å³°å€¼
                freq_range = 5  # Hz
                mask = (frequencies >= harmonic_freq - freq_range) & (frequencies <= harmonic_freq + freq_range)
                if np.any(mask):
                    harmonic_idx = np.argmax(fft_magnitude[mask])
                    actual_idx = np.where(mask)[0][harmonic_idx]
                    harmonics.append({
                        'order': h,
                        'frequency': float(frequencies[actual_idx]),
                        'amplitude': float(fft_magnitude[actual_idx])
                    })
        
        # ç†è®ºæ•…éšœé¢‘ç‡
        theoretical_freqs = self.calculate_theoretical_fault_frequencies(rpm, bearing_type)
        
        analysis_result = {
            'main_frequency': float(main_frequency),
            'main_amplitude': float(main_amplitude),
            'harmonics': harmonics,
            'theoretical_frequencies': theoretical_freqs,
            'peak_count': len(peak_indices)
        }
        
        return analysis_result
    
    def plot_time_domain(self, segment_data, fs, output_path, segment_name):
        """ç»˜åˆ¶æ—¶åŸŸä¿¡å·"""
        plt.rcParams['font.sans-serif'] = ['SimHei']
        plt.rcParams['axes.unicode_minus'] = False
        
        data = segment_data.flatten()
        time = np.arange(len(data)) / fs
        
        fig, ax = plt.subplots(figsize=(12, 6))
        ax.plot(time, data, 'b-', linewidth=0.8)
        ax.set_xlabel('æ—¶é—´ (ç§’)')
        ax.set_ylabel('æŒ¯å¹…')
        ax.set_title(f'{segment_name} - æ—¶åŸŸä¿¡å·')
        ax.grid(True, alpha=0.3)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        rms = np.sqrt(np.mean(data**2))
        peak = np.max(np.abs(data))
        peak2peak = np.max(data) - np.min(data)
        
        info_text = f'RMS: {rms:.4f}\nPeak: {peak:.4f}\nP-P: {peak2peak:.4f}\nç‚¹æ•°: {len(data):,}'
        ax.text(0.02, 0.98, info_text, transform=ax.transAxes, 
                verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
        
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_frequency_domain(self, fft_magnitude, frequencies, output_path, segment_name, freq_analysis):
        """ç»˜åˆ¶é¢‘åŸŸä¿¡å·"""
        plt.rcParams['font.sans-serif'] = ['SimHei']
        plt.rcParams['axes.unicode_minus'] = False
        
        fig, ax = plt.subplots(figsize=(12, 6))
        ax.plot(frequencies, fft_magnitude, 'b-', linewidth=0.8)
        ax.set_xlabel('é¢‘ç‡ (Hz)')
        ax.set_ylabel('å¹…å€¼')
        ax.set_title(f'{segment_name} - é¢‘åŸŸä¿¡å·')
        ax.grid(True, alpha=0.3)
        
        # æ ‡æ³¨ä¸»é¢‘
        main_freq = freq_analysis['main_frequency']
        main_amp = freq_analysis['main_amplitude']
        ax.plot(main_freq, main_amp, 'ro', markersize=8)
        ax.annotate(f'ä¸»é¢‘: {main_freq:.1f}Hz', 
                   xy=(main_freq, main_amp), xytext=(main_freq+50, main_amp),
                   arrowprops=dict(arrowstyle='->', color='red'))
        
        # æ ‡æ³¨è°æ³¢
        for harmonic in freq_analysis['harmonics'][:3]:  # åªæ ‡æ³¨å‰3ä¸ªè°æ³¢
            h_freq = harmonic['frequency']
            h_amp = harmonic['amplitude']
            ax.plot(h_freq, h_amp, 'go', markersize=6)
            ax.annotate(f'{harmonic["order"]}æ¬¡è°æ³¢', 
                       xy=(h_freq, h_amp), xytext=(h_freq+30, h_amp*0.8),
                       arrowprops=dict(arrowstyle='->', color='green'))
        
        # æ ‡æ³¨ç†è®ºæ•…éšœé¢‘ç‡
        theoretical = freq_analysis['theoretical_frequencies']
        colors = {'BPFO': 'orange', 'BPFI': 'purple', 'BSF': 'brown', 'FTF': 'pink'}
        for fault_type, freq_val in theoretical.items():
            if fault_type != 'FR' and freq_val < max(frequencies):
                ax.axvline(x=freq_val, color=colors.get(fault_type, 'gray'), 
                          linestyle='--', alpha=0.7, label=f'{fault_type}: {freq_val:.1f}Hz')
        
        ax.legend()
        plt.tight_layout()
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
    
    def process_single_file(self, file_path, category):
        """
        å¤„ç†å•ä¸ªæ–‡ä»¶ï¼Œä¸¥æ ¼ç”Ÿæˆ3ä¸ªæ•°æ®ç‰‡æ®µ
        """
        print(f"\nğŸ“„ å¤„ç†æ–‡ä»¶: {file_path.name}")
        
        # åŠ è½½æ•°æ®
        mat_data = self.load_mat_file(file_path)
        if mat_data is None:
            return []
        
        # ç¡®å®šé‡‡æ ·é¢‘ç‡å’Œé™é‡‡æ ·ç­–ç•¥
        if '48kHz' in category:
            original_fs = 48000
            target_fs = 12000
            need_downsample = True
        else:
            original_fs = 12000
            target_fs = 12000
            need_downsample = False
        
        # æŸ¥æ‰¾æ•°æ®å˜é‡
        time_vars = [k for k in mat_data.keys() if not k.startswith('__') and 'time' in k.lower()]
        rpm_vars = [k for k in mat_data.keys() if not k.startswith('__') and 'rpm' in k.lower()]
        
        if not time_vars:
            print(f"âŒ æœªæ‰¾åˆ°æ—¶åŸŸæ•°æ®å˜é‡")
            return []
        
        # è·å–RPM
        rpm = 1760  # é»˜è®¤è½¬é€Ÿ
        if rpm_vars:
            rpm_data = mat_data[rpm_vars[0]]
            if isinstance(rpm_data, np.ndarray):
                rpm = float(rpm_data.flatten()[0])
        
        processed_segments = []
        
        # åªå¤„ç†ç¬¬ä¸€ä¸ªä¸»è¦çš„æ—¶åŸŸå˜é‡ï¼ˆé€šå¸¸æ˜¯DE_timeï¼‰
        primary_var = time_vars[0]
        for var_name in time_vars:
            if 'DE_time' in var_name:
                primary_var = var_name
                break
        
        data = mat_data[primary_var]
        if not isinstance(data, np.ndarray) or len(data.shape) < 1:
            print(f"âŒ æ•°æ®æ ¼å¼é”™è¯¯: {primary_var}")
            return []
        
        print(f"  å¤„ç†å˜é‡: {primary_var}, é•¿åº¦: {len(data):,}")
        
        # é™é‡‡æ ·ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if need_downsample:
            data = self.downsample_signal(data, original_fs, target_fs)
            print(f"  é™é‡‡æ ·åé•¿åº¦: {len(data):,}")
        
        # å»å™ª
        data_denoised = self.apply_denoising(data, target_fs)
        
        # æå–3ä¸ªç‰‡æ®µï¼Œç¡®ä¿é«˜åˆ©ç”¨ç‡
        segments, utilization_rate = self.extract_three_segments_with_high_utilization(data_denoised, target_fs)
        
        # ç¡®ä¿æ­£å¥½æœ‰3ä¸ªç‰‡æ®µ
        if len(segments) != 3:
            print(f"âš ï¸ è­¦å‘Šï¼šæœŸæœ›3ä¸ªç‰‡æ®µï¼Œå®é™…å¾—åˆ°{len(segments)}ä¸ª")
            # å¼ºåˆ¶è°ƒæ•´ä¸º3ä¸ªç‰‡æ®µ
            while len(segments) < 3:
                # å¤åˆ¶æœ€åä¸€ä¸ªæ®µ
                segments.append(segments[-1] if segments else data_denoised)
            segments = segments[:3]  # åªå–å‰3ä¸ª
        
        # å¤„ç†æ¯ä¸ªç‰‡æ®µ
        for seg_idx, segment in enumerate(segments):
            # ç”Ÿæˆå”¯ä¸€çš„ç‰‡æ®µID
            segment_id = f"{file_path.stem}_{seg_idx+1}_{self.global_folder_counter:03d}"
            self.global_folder_counter += 1
            
            # ç‰¹å¾æå–
            time_features = self.extract_time_features(segment)
            freq_features, fft_magnitude, frequencies = self.compute_fft_features(segment, target_fs)
            
            # é¢‘ç‡åˆ†æ
            bearing_type = 'SKF6205' if 'DE' in primary_var else 'SKF6203'
            freq_analysis = self.analyze_frequency_components(fft_magnitude, frequencies, target_fs, rpm, bearing_type)
            
            # ç¡®å®šæ•…éšœç±»å‹
            fault_type = self._determine_fault_type(file_path.name, category)
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            segment_dir = self.output_base / f"{segment_id}"
            segment_dir.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜åŸå§‹æ•°æ®
            np.save(segment_dir / f"{segment_id}_raw_data.npy", segment)
            
            # ç»˜åˆ¶å›¾åƒ
            self.plot_time_domain(segment, target_fs, 
                                segment_dir / f"{segment_id}_time_domain.png", segment_id)
            self.plot_frequency_domain(fft_magnitude, frequencies, 
                                     segment_dir / f"{segment_id}_frequency_domain.png", 
                                     segment_id, freq_analysis)
            
            # ä¿å­˜ç‰¹å¾
            all_features = {**time_features, **freq_features, 'fault_type': fault_type}
            features_df = pd.DataFrame([all_features])
            features_df.to_csv(segment_dir / f"{segment_id}_features.csv", index=False, encoding='utf-8-sig')
            
            # ä¿å­˜é¢‘ç‡åˆ†æ
            freq_df = pd.DataFrame([freq_analysis])
            freq_df.to_csv(segment_dir / f"{segment_id}_frequency_analysis.csv", index=False, encoding='utf-8-sig')
            
            processed_segments.append({
                'segment_id': segment_id,
                'original_file': str(file_path),
                'variable': primary_var,
                'segment_index': seg_idx + 1,
                'original_length': len(data),
                'processed_length': len(segment),
                'fault_type': fault_type,
                'rpm': rpm,
                'sampling_rate': target_fs,
                'utilization_rate': utilization_rate
            })
            
            self.processed_count += 1
            print(f"    âœ… ç‰‡æ®µ {seg_idx+1}: {segment_id} (é•¿åº¦: {len(segment):,})")
        
        return processed_segments
    
    def _determine_fault_type(self, filename, category):
        """ç¡®å®šæ•…éšœç±»å‹"""
        filename_upper = filename.upper()
        if 'OR' in filename_upper or 'OUTER' in filename_upper:
            return 'OR'
        elif 'IR' in filename_upper or 'INNER' in filename_upper:
            return 'IR'
        elif 'B' in filename_upper and any(x in filename for x in ['007', '014', '021', '028']):
            return 'B'
        elif 'N' in filename_upper or 'NORMAL' in filename_upper:
            return 'N'
        else:
            return 'Unknown'
    
    # è¾…åŠ©å‡½æ•°
    def _skewness(self, data):
        """è®¡ç®—ååº¦"""
        return np.mean(((data - np.mean(data)) / np.std(data)) ** 3)
    
    def _kurtosis(self, data):
        """è®¡ç®—å³­åº¦"""
        return np.mean(((data - np.mean(data)) / np.std(data)) ** 4) - 3
    
    def _calculate_entropy(self, data):
        """è®¡ç®—ç†µ"""
        hist, _ = np.histogram(data, bins=50)
        hist = hist[hist > 0]
        prob = hist / np.sum(hist)
        return -np.sum(prob * np.log2(prob))
    
    def _mean_frequency(self, data):
        """è®¡ç®—å¹³å‡é¢‘ç‡"""
        fft_data = np.fft.fft(data)
        magnitude = np.abs(fft_data[:len(fft_data)//2])
        freqs = np.arange(len(magnitude))
        return np.sum(freqs * magnitude) / np.sum(magnitude)
    
    def _freq_skewness(self, frequencies, magnitude):
        """é¢‘åŸŸååº¦"""
        mean_freq = np.sum(frequencies * magnitude) / np.sum(magnitude)
        variance = np.sum((frequencies - mean_freq)**2 * magnitude) / np.sum(magnitude)
        if variance == 0:
            return 0
        std_freq = np.sqrt(variance)
        return np.sum(((frequencies - mean_freq) / std_freq)**3 * magnitude) / np.sum(magnitude)
    
    def _freq_kurtosis(self, frequencies, magnitude):
        """é¢‘åŸŸå³­åº¦"""
        mean_freq = np.sum(frequencies * magnitude) / np.sum(magnitude)
        variance = np.sum((frequencies - mean_freq)**2 * magnitude) / np.sum(magnitude)
        if variance == 0:
            return 0
        std_freq = np.sqrt(variance)
        return np.sum(((frequencies - mean_freq) / std_freq)**4 * magnitude) / np.sum(magnitude) - 3
    
    def _spectral_rolloff(self, frequencies, magnitude, rolloff_percent=0.85):
        """è°±æ»šé™"""
        total_energy = np.sum(magnitude)
        rolloff_energy = rolloff_percent * total_energy
        cumulative_energy = np.cumsum(magnitude)
        rolloff_idx = np.where(cumulative_energy >= rolloff_energy)[0]
        return frequencies[rolloff_idx[0]] if len(rolloff_idx) > 0 else frequencies[-1]
    
    def _spectral_flatness(self, magnitude):
        """è°±å¹³å¦åº¦"""
        geometric_mean = np.exp(np.mean(np.log(magnitude + 1e-10)))
        arithmetic_mean = np.mean(magnitude)
        return geometric_mean / arithmetic_mean if arithmetic_mean > 0 else 0
    
    def _spectral_bandwidth(self, frequencies, magnitude):
        """è°±å¸¦å®½"""
        centroid = np.sum(frequencies * magnitude) / np.sum(magnitude)
        return np.sqrt(np.sum((frequencies - centroid)**2 * magnitude) / np.sum(magnitude))


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ æœ€ç»ˆæ•°æ®å¤„ç†å™¨ - ä¸¥æ ¼æŒ‰ç…§è¦æ±‚å®ç°")
    print("=" * 60)
    print("ç›®æ ‡: 161ä¸ªæ–‡ä»¶ â†’ 483ä¸ªæ•°æ®ç‰‡æ®µ (æ¯ä¸ªæ–‡ä»¶3ä¸ª)")
    print("è¦æ±‚: ç‰‡æ®µåˆ©ç”¨ç‡â‰¥60%ï¼Œä¿è¯æ•°æ®å¯¹é½å’Œè´¨é‡")
    print("=" * 60)
    
    # è·¯å¾„è®¾ç½®
    source_base = "/Users/gsyoung/MBP_documents/code/Matlab/ç ”ç©¶ç”Ÿæ•°å­¦å»ºæ¨¡/æ•°æ®é›†/æ•°æ®é›†/æºåŸŸæ•°æ®é›†"
    output_base = "/Users/gsyoung/MBP_documents/code/Matlab/ç ”ç©¶ç”Ÿæ•°å­¦å»ºæ¨¡/å»ºæ¨¡å®ç°/å¤„ç†åæ•°æ®/final_segments"
    
    # åˆ›å»ºå¤„ç†å™¨
    processor = FinalDataProcessor(source_base, output_base)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(output_base).mkdir(parents=True, exist_ok=True)
    
    all_processed_segments = []
    processing_stats = {}
    total_files_processed = 0
    total_utilization_rates = []
    
    # å¤„ç†æ‰€æœ‰ç±»åˆ«
    categories = ['12kHz_DE_data', '12kHz_FE_data', '48kHz_DE_data', '48kHz_Normal_data']
    
    for category in categories:
        category_path = Path(source_base) / category
        if not category_path.exists():
            print(f"âš ï¸ ç±»åˆ«è·¯å¾„ä¸å­˜åœ¨: {category_path}")
            continue
        
        print(f"\nğŸ“ å¤„ç†ç±»åˆ«: {category}")
        
        # æŸ¥æ‰¾æ‰€æœ‰.matæ–‡ä»¶
        mat_files = list(category_path.rglob("*.mat"))
        print(f"æ‰¾åˆ° {len(mat_files)} ä¸ª.matæ–‡ä»¶")
        
        category_segments = []
        category_utilization_rates = []
        
        for file_idx, mat_file in enumerate(mat_files):
            segments = processor.process_single_file(mat_file, category)
            category_segments.extend(segments)
            all_processed_segments.extend(segments)
            total_files_processed += 1
            
            # æ”¶é›†åˆ©ç”¨ç‡æ•°æ®
            if segments:
                file_utilization = segments[0]['utilization_rate']  # æ‰€æœ‰ç‰‡æ®µçš„åˆ©ç”¨ç‡ç›¸åŒ
                category_utilization_rates.append(file_utilization)
                total_utilization_rates.append(file_utilization)
        
        avg_utilization = np.mean(category_utilization_rates) if category_utilization_rates else 0
        
        processing_stats[category] = {
            'input_files': len(mat_files),
            'output_segments': len(category_segments),
            'segments_per_file': len(category_segments) / len(mat_files) if len(mat_files) > 0 else 0,
            'avg_utilization_rate': avg_utilization
        }
        
        print(f"ğŸ“Š {category} ç»Ÿè®¡:")
        print(f"  è¾“å…¥æ–‡ä»¶: {len(mat_files)} ä¸ª")
        print(f"  è¾“å‡ºç‰‡æ®µ: {len(category_segments)} ä¸ª")
        print(f"  å¹³å‡æ¯æ–‡ä»¶ç‰‡æ®µæ•°: {len(category_segments) / len(mat_files):.1f}")
        print(f"  å¹³å‡åˆ©ç”¨ç‡: {avg_utilization:.1%}")
    
    # æ€»ä½“ç»Ÿè®¡
    overall_utilization = np.mean(total_utilization_rates) if total_utilization_rates else 0
    
    print(f"\nğŸ¯ æ€»ä½“ç›®æ ‡è¾¾æˆæƒ…å†µ:")
    print(f"  ç›®æ ‡æ–‡ä»¶æ•°: 161 ä¸ª")
    print(f"  å®é™…å¤„ç†: {total_files_processed} ä¸ª")
    print(f"  ç›®æ ‡ç‰‡æ®µæ•°: 483 ä¸ª (161Ã—3)")
    print(f"  å®é™…ç”Ÿæˆ: {len(all_processed_segments)} ä¸ª")
    print(f"  è¾¾æˆç‡: {len(all_processed_segments)/483*100:.1f}%")
    print(f"  å¹³å‡åˆ©ç”¨ç‡: {overall_utilization:.1%} (è¦æ±‚â‰¥60%)")
    
    # åˆ©ç”¨ç‡æ£€æŸ¥
    utilization_pass = overall_utilization >= 0.6
    segment_count_pass = len(all_processed_segments) == 483
    
    print(f"\nâœ… è´¨é‡æ£€æŸ¥:")
    print(f"  ç‰‡æ®µæ•°é‡: {'âœ… é€šè¿‡' if segment_count_pass else 'âŒ æœªè¾¾æ ‡'}")
    print(f"  åˆ©ç”¨ç‡è¦æ±‚: {'âœ… é€šè¿‡' if utilization_pass else 'âŒ æœªè¾¾æ ‡'}")
    
    # ä¿å­˜å¤„ç†ç»Ÿè®¡
    stats_summary = {
        'processing_time': datetime.now().isoformat(),
        'target_files': 161,
        'processed_files': total_files_processed,
        'target_segments': 483,
        'generated_segments': len(all_processed_segments),
        'target_utilization': 0.6,
        'actual_utilization': overall_utilization,
        'category_stats': processing_stats,
        'segments_detail': all_processed_segments,
        'quality_check': {
            'segment_count_pass': segment_count_pass,
            'utilization_pass': utilization_pass
        }
    }
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    reports_dir = Path(output_base).parent / "reports"
    reports_dir.mkdir(exist_ok=True)
    
    with open(reports_dir / "final_processing_report.json", 'w', encoding='utf-8') as f:
        json.dump(stats_summary, f, ensure_ascii=False, indent=2, default=str)
    
    # ä¿å­˜è¯¦ç»†ç»Ÿè®¡CSV
    segments_df = pd.DataFrame(all_processed_segments)
    segments_df.to_csv(reports_dir / "final_segments_summary.csv", index=False, encoding='utf-8-sig')
    
    if segment_count_pass and utilization_pass:
        print(f"\nğŸ‰ ä»»åŠ¡å®Œæˆï¼æˆåŠŸç”Ÿæˆ483ä¸ªé«˜è´¨é‡æ•°æ®ç‰‡æ®µ")
    else:
        print(f"\nâš ï¸ ä»»åŠ¡éœ€è¦è°ƒæ•´ï¼Œè¯·æ£€æŸ¥è´¨é‡æŒ‡æ ‡")
    
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_base}")
    print(f"ğŸ“Š è¯¦ç»†æŠ¥å‘Š: final_processing_report.json")

if __name__ == "__main__":
    main()
